{
  
    
        "post0": {
            "title": "Disc Golf Disc Classifier",
            "content": "Live App is running at https://udisc.mattlichti.com/ . All the data was submitted through the Udisc Android &amp; iPhone apps https://udisc.com/ . The code is based on lessons 1 &amp; 2 of fastai course 3 https://course.fast.ai/ . %reload_ext autoreload %autoreload 2 %matplotlib inline from fastai.vision import * from fastai.metrics import error_rate import urllib.request as req import pandas as pd import os import numpy as np . Step 1: Cleaning the Data and loading it into the fastai ImageDataBunch class . df = pd.read_csv(&#39;transformed_97_discs.csv&#39;, index_col=0) . df = df.loc[df.downloaded] df = df.loc[df[&#39;size&#39;]&gt;=25000] df.shape . (63815, 13) . np.random.seed(42) df = df[df[&#39;discName&#39;].isin([&#39;Leopard&#39;, &#39;Buzzz&#39;, &#39;Firebird&#39;])] df = df.sample(3000) df.groupby(&#39;discName&#39;).count() . manufacturerName image plastic created type speed size downloaded disc_label plastic_label folder path . discName . Buzzz 1168 | 1168 | 967 | 1168 | 1158 | 839 | 1168 | 1168 | 1168 | 1168 | 1168 | 1168 | . Firebird 786 | 786 | 755 | 786 | 778 | 715 | 786 | 786 | 786 | 786 | 786 | 786 | . Leopard 1046 | 1046 | 834 | 1046 | 1040 | 1006 | 1046 | 1046 | 1046 | 1046 | 1046 | 1046 | . df.index = df.path df = df.loc[:, [&#39;disc_label&#39;]] df.to_csv(&#39;disc_images/labels.csv&#39;) . np.random.seed(42) path = Path(&#39;/home/jupyter/disc_classifier/disc_images&#39;) data = ImageDataBunch.from_csv(path, valid_pct=.1, ds_tfms=get_transforms(do_flip=False), size=224, num_workers=4).normalize(imagenet_stats) . data.classes, len(data.train_ds), len(data.valid_ds) . ([&#39;Discraft Buzzz&#39;, &#39;Innova Firebird&#39;, &#39;Innova Leopard&#39;], 2700, 300) . data.show_batch() . Step 2: Training the Convolutional Neural Net. We use a 50 layer ResNet that has been pretrained on ImageNet. https://en.wikipedia.org/wiki/Residual_neural_network . learn = cnn_learner(data, models.resnet50, metrics=error_rate) . learn.lr_find() learn.recorder.plot() . LR Finder is complete, type {learner_name}.recorder.plot() to see the graph. . learn.fit_one_cycle(4, max_lr=3*1e-3) . Total time: 03:27 epoch train_loss valid_loss error_rate time . 0 0.801391 0.698489 0.223333 00:53 . 1 0.523101 0.402900 0.143333 00:51 . 2 0.348445 0.353320 0.160000 00:51 . 3 0.239959 0.340474 0.140000 00:51 . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; learn.save(&#39;3_discs-1-4&#39;) . Step 3: Interpreting the Results . interp = ClassificationInterpretation.from_learner(learn) . interp.plot_confusion_matrix(figsize=(5,5)) . interp.plot_top_losses(16, figsize=(25,25), heatmap=False) . Step 4 (optional): Train more layers of the neural net to improve performance . learn.unfreeze() learn.lr_find() learn.recorder.plot() . LR Finder is complete, type {learner_name}.recorder.plot() to see the graph. . learn.fit_one_cycle(3, max_lr=slice(5*1e-5,3*1e-4)) . Total time: 02:34 epoch train_loss valid_loss error_rate time . 0 0.179046 0.387953 0.136667 00:52 . 1 0.142624 0.331104 0.103333 00:51 . 2 0.090960 0.257891 0.096667 00:50 . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; learn.save(&#39;3_discs-2-3&#39;) . interp = ClassificationInterpretation.from_learner(learn) . interp.plot_confusion_matrix(figsize=(5,5)) . Step 5: Putting in to production . defaults.device = torch.device(&#39;cpu&#39;) . path = Path(&#39;/home/jupyter/disc_classifier/disc_images&#39;) data = ImageDataBunch.from_csv(path, valid_pct=.1, ds_tfms=get_transforms(flip_vert=True), size=224, num_workers=4).normalize(imagenet_stats) learn = cnn_learner(data, models.resnet50, metrics=error_rate) . . path = Path(&#39;/home/jupyter/disc_classifier/&#39;) leopard = open_image(path/&#39;leopard.jpeg&#39;) . leopard = open_image(path/&#39;leopard.jpeg&#39;) . leopard . pred_class,pred_idx,outputs = learn.predict(leopard) . pred_class . Category Innova Leopard . outputs . tensor([1.3064e-05, 7.1869e-04, 9.9927e-01]) . data.classes . [&#39;Discraft Buzzz&#39;, &#39;Innova Firebird&#39;, &#39;Innova Leopard&#39;] . learn.export(&#39;3discs.pkl&#39;) . def predict_disc(learn, img): pred_class,pred_idx,outputs = learn.predict(img) idx = np.argsort(-outputs)[0] if outputs[idx].item()&gt;.75: output = &#39;We think your disc is: &#39; + str(pred_class) else: output = &quot;Sorry, we aren&#39;t sure what kind of disc that is.&quot; output += &#39; nTop disc mold probabilities for your disc: n&#39; for idx in np.argsort(-outputs)[:10]: if outputs[idx].item()&gt;.00005: output += str(data.classes[idx]) + &#39;: &#39; output += str(round(outputs[idx].item()*100,2)) + &#39;%&#39; + &#39; n&#39; return(output) . print(predict_disc(learn, leopard)) . We think your disc is: Innova Leopard Top disc mold probabilities for your disc: Innova Leopard: 99.93% Innova Firebird: 0.07% . firebird = open_image(path/&#39;firebird.jpg&#39;) . firebird . print(predict_disc(learn, firebird)) . We think your disc is: Innova Leopard Top disc mold probabilities for your disc: Innova Leopard: 97.34% Innova Firebird: 1.98% Discraft Buzzz: 0.68% . &lt;/div&gt; .",
            "url": "https://mattlichti.github.io/blog/jupyter/2019/03/27/disc-classifier.html",
            "relUrl": "/jupyter/2019/03/27/disc-classifier.html",
            "date": " • Mar 27, 2019"
        }
        
    
  
    
        ,"post1": {
            "title": "How to get loans funded on kiva",
            "content": "How to get loans funded on kiva . This project is an analysis of what features make microfinance loans on kiva.org more likely to get funded. The full analysis and explanation of my process is here . Motivation: . There are over 2 million small time lenders on kiva who have funded over 880,000 microfinance loans since 2005. The loans are originally made by 296 different microfinance organizations who use kiva to raise money to backfill some of their loans. Lenders can view the loans on the website and lend as little as $25 to any of the borrowers they chose. Loans usually have a 30 day expiration policy, which means that loans that are not fully funded in that time are refunded and the microfinance organization does not receive any money. Because of this, it is important for the microfinance organizations to understand the characteristics that make loans more likely to get funded. . Data: . Kiva makes their loan data available through their api. They also periodically make downloadable snapshots of the data. I most recently used the May 18 2015 json snapshot for this analysis. The loan data is in a 1 GB zip file that is 5 GB when unzipped. . Pipeline: . data_pipeline.py is used for processing the raw kiva loan data. It extracts the relevant data, performs the feature engineering, and then stores it all in a postgres SQL database. To run the pipeline, unzip the loans folder containing around 1800 json files that each have the data from 500 loans. Then setup a postgres database and run pipeline.py in the terminal the loans folder location and sql information as command line arguments. The most important part of the process is feature engineering. The features include anything that a potential borrower sees when viewing a loan on kiva org that they can use to decide whether or not to loan to a particular individual. . Modeling: . build_model.py is used to train the model, predict which loans have a higher risk of expiring, and determine which features are most important in predicting loan success. The model converts the categorical features into boolean dummy variables, and tokenizes, lemmatizes, and performs TF-IDF on the text. I used a random forest model. The classes were unbalanced with a much higher number of funded loans than expired loans, so I heavily weighted the expired loans in order to increase recall of expired loans at the expense of precision. The model can output a confusion matrix and a list of feature importance which could be used as recommendations on how to improve their odds of getting their loans funded. . Running the model . run_model.py is used to load the relevant data from the postgres sql database and run the model on that data. It can be run from the command line like data_pipeline.py. . Plotting . plots.py is used to make plots of the feature importance of the most important features in the random forest model, as well as plot the expiration rate based on a variety of features and the expiration rate over time. .",
            "url": "https://mattlichti.github.io/blog/markdown/2015/06/01/kiva-funding-model.html",
            "relUrl": "/markdown/2015/06/01/kiva-funding-model.html",
            "date": " • Jun 1, 2015"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "2015 Data Science Website . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://mattlichti.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://mattlichti.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}